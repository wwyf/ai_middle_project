{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'E:/0code')\n",
    "# sys.path.append(r'/home/wangyf226/0code')\n",
    "# sys.path.append(r'/BIGDATA1/nsccgz_yfdu_1/asc19/wyf/pyml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gensim\n",
    "import re\n",
    "import smart_open\n",
    "from pyml.feature_extraction.text import CountVectorizer\n",
    "from pyml.linear_model.classification import LogisticClassifier\n",
    "from pyml.neighbors.classification import KNeighborsClassifier\n",
    "from pyml.metrics.classification import precision_score\n",
    "from pyml.model_selection import KFold\n",
    "from pyml.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 读取训练数据和测试数据为字符串的列表\n",
    "2. 读取训练集label，并转换为数字格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_text_to_list(filename):\n",
    "    with open(filename, 'r', encoding='UTF-8') as f:\n",
    "        contents = f.readlines()\n",
    "    lines = [l.strip() for l in contents]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ori_X = read_train_text_to_list('../data/trainData.txt')\n",
    "train_ori_Y = read_train_text_to_list('../data/trainLabel.txt')\n",
    "train_ori_Y = np.array([int(y) for y in train_ori_Y])\n",
    "# test_ori_X = read_train_text_to_list('../data/testData.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_documents(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"UTF-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                # 变小写，去标点符号，分词\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = list(read_raw_documents('../data/trainData.txt'))\n",
    "test_sentences = list(read_raw_documents('../data/testData.txt', tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理 & 特征工程\n",
    "1. Count Vectors as feature\n",
    "2. TF-IDF Vectors as festures\n",
    "3. Word Embeddings as features\n",
    "4. Text/NLP based features\n",
    "5. Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = len(train_sentences)\n",
    "n_test_samples = len(test_sentences)\n",
    "vector_size = 50\n",
    "train_X = np.zeros((n_train_samples, vector_size))\n",
    "test_X = np.zeros((n_test_samples, vector_size))\n",
    "for i in range(0, n_train_samples):\n",
    "    train_X[i] = model.infer_vector(train_sentences[i][0])\n",
    "for i in range(0, n_test_samples):\n",
    "    test_X[i] = model.infer_vector(test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_ori_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉验证-寻找最好的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56 0.56 0.54 0.55 0.54 0.55 0.54 0.48 0.57 0.52 0.51 0.5  0.54 0.53\n",
      " 0.53 0.53 0.53 0.53 0.53 0.53]\n",
      "best k  9\n"
     ]
    }
   ],
   "source": [
    "k_range = range(1,21)\n",
    "k_splits = 10\n",
    "ms = KFold(k_splits=k_splits)\n",
    "k_scores = np.zeros((len(k_range)))\n",
    "for train_indices, test_indices in ms.split(train_X):\n",
    "    for k in k_range:\n",
    "        clf = KNeighborsClassifier(k=k)\n",
    "        clf.fit(train_X[train_indices], train_Y[train_indices])\n",
    "        y_pred = clf.predict(train_X[test_indices])\n",
    "        score = precision_score(train_Y[test_indices], y_pred)\n",
    "#         print('k : {} score: {}'.format(k, score))\n",
    "        k_scores[k_range.index(k)] += score\n",
    "avg_k_scores = k_scores/k_splits\n",
    "print(avg_k_scores)\n",
    "print(\"best k \", np.argmax(avg_k_scores)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型写入结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(k=6)\n",
    "clf.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_X)\n",
    "sub = pd.DataFrame(y_pred)\n",
    "sub.to_csv('../results/'+'KNN-'+ str(datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")) + \".csv\", index=0, header=None, index_label=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
